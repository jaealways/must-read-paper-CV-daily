# must-read-paper-CV-daily
List of papers with recent trend in CV

## 2023
- [2023 CVPR](/2023/CVPR.md)



## Updated on 2024.02.14

# Must-read LIST

|Date|Title|Authors|PDF|TAGS|TLDR|
|---|---|---|---|---|---|
|**02-24**|**CREPE: Can Vision-Language Foundation Models Reason Compositionally?**|Z Ma et.al.|[2212.07796](https://arxiv.org/pdf/2212.07796.pdf)|``CVPR'23``|**[](./TLDR/CREPE-Can-Vision-Language-Foundation-Models-Reason-Compositionally.md)**|
|**02-23**|**DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models**|G Zheng et.al.|[2310.16436](https://arxiv.org/pdf/2310.16436.pdf)|``NeurIPS'23``, `multi-modal`|**[](./TLDR/DDCoT-Duty-Distinct-Chain-of-Thought-Prompting-for-Multimodal-Reasoning-in-Language-Models.md)**|
|**02-22**|**Multimodal Few-Shot Learning with Frozen Language Models**|M Tsimpoukelli et.al.|[2106.13884](https://arxiv.org/pdf/2106.13884.pdf)|``NeurIPS'21``, `Deepmind`, `multi-modal`|**[](./TLDR/Multimodal-Few-Shot-Learning-with-Frozen-Language-Models.md)**|
|**02-21**|**Flamingo: a Visual Language Model for Few-Shot Learning**|JB Alayrac et.al.|[2204.14198](https://arxiv.org/pdf/2204.14198.pdf)|`NeurIPS'22`, `Deepmind`, `multi-modal`|**[](./TLDR/Flamingo-a-Visual-Language-Model-for-Few-Shot-Learning.md)**|
|**02-20**|**An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA**|Z Yang et.al.|[2109.05014](https://arxiv.org/pdf/2109.05014.pdf)|`AAAI'22`, `Microsoft`, `multi-modal`|**[](./TLDR/An-Empirical-Study-of-GPT-3-for-Few-Shot-Knowledge-Based-VQA.md)**|
|**02-19**|**Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering**|Z Shao et.al.|[2303.01903](https://arxiv.org/pdf/2303.01903.pdf)|`CVPR'23`, `multi-modal`|**[](./TLDR/Prompting-Large-Language-Models-with-Answer-Heuristics-for-Knowledge-based-Visual-Question-Answering.md)**|
|**02-18**|**ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction**|J He et.al.|[2303.05063](https://arxiv.org/pdf/2303.05063.pdf)|`ICCV'23`, `multi-modal`|**[](./TLDR/ICL-D3IE-In-Context-Learning-with-Diverse-Demonstrations-Updating-for-Document-Information-Extraction.md)**|
|**02-17**|**Exploring Diverse In-Context Configurations for Image Captioning**|X Yang et.al.|[2305.14800](https://arxiv.org/pdf/2305.14800.pdf)|`NeurIPS'23`, `multi-modal`|**[](./TLDR/Exploring-Diverse-In-Context-Configurations-for-Image-Captioning.md)**|
|**02-16**|**MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning**|Z Xu et.al.|[2212.10773](https://arxiv.org/pdf/2212.10773.pdf)|`ACL'23`, `multi-modal`|**[](./TLDR/MultiInstruct-Improving-Multi-Modal-Zero-Shot-Learning-via-Instruction-Tuning.md)**|
|**02-15**|**LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention**|R Zhang et.al.|[2303.16199](https://arxiv.org/pdf/2303.16199.pdf)|`ICLR'24`, `multi-modal`|**[](./TLDR/LLaMA-Adapter-Efficient-Fine-tuning-of-Language-Models-with-Zero-init-Attention.md)**|
|**02-14**|**Visual Instruction Tuning**|H Liu et.al.|[2304.08485](https://arxiv.org/pdf/2304.08485.pdf)|`NeurIPS'23`|**[](./TLDR/Visual-Instruction-Tuning.md)**|
|**02-13**|**Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages**|J Hu et.al.|[2308.12038](https://arxiv.org/pdf/2308.12038.pdf)|`ICLR'24`|**[](./TLDR/Large-Multilingual-Models-Pivot-Zero-Shot-Multimodal-Learning-across-Languages.md)**|
|**02-12**|**LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment**|B Zhu et.al.|[2310.01852](https://arxiv.org/pdf/2310.01852.pdf)|`ICLR'24`|**[](./TLDR/LanguageBind-Extending-Video-Language-Pretraining-to-N-modality-by-Language-based-Semantic-Alignment.md)**|
|**02-11**|**Text-to-Image Diffusion Models are Zero-Shot Classifiers**|K Clark et.al.|[2303.15233](https://arxiv.org/pdf/2303.15233.pdf)|`ICLR'23`|**[](./TLDR/Text-to-Image-Diffusion-Models-are-Zero-Shot-Classifiers.md)**|
|**02-10**|**MMVAE+: Enhancing the Generative Quality of Multimodal VAEs without Compromises**|E Palumbo et.al.|[B42UJTVdDZ5](https://openreview.net/pdf?id=B42UJTVdDZ5)|`ICLR'23`|**[](./TLDR/MMVAE+-Enhancing-the-Generative-Quality-of-Multimodal-VAEs-without-Compromises.md)**|
|**02-09**|**Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study**|M Hendriksen et.al.|[2301.05174](https://arxiv.org/pdf/2301.05174.pdf)|`ECIR'23`|**[](./TLDR/Scene-centric-vs.-Object-centric-Image-Text-Cross-modal-Retrieval-A-Reproducibility-Study.md)**|
|**02-08**|**Identifiability Results for Multimodal Contrastive Learning**|I Daunhawer et.al.|[2303.09166](https://arxiv.org/pdf/2303.09166.pdf)|`ICLR'23`|**[](./TLDR/Identifiability-Results-for-Multimodal-Contrastive-Learning.md)**|
|**02-07**|**UniCLIP: Unified Framework for Contrastive Language Image Pre training**|J Lee et.al.|[2209.13430](https://arxiv.org/pdf/2209.13430.pdf)|`NeurIPS'22` |**[](./TLDR/UniCLIP-Unified-Framework-for-Contrastive-Language-Image-Pre-training.md)**|
|**02-06**|**ProtoCon: Pseudo-label Refinement via Online Clustering and Prototypical Consistency for Efficient Semi-supervised Learning**|I Nassar et.al.|[2303.13556](https://arxiv.org/pdf/2303.13556.pdf)|`CVPR'23`|**[](./TLDR/ProtoCon-Pseudo-label-Refinement-via-Online-Clustering-and-Prototypical-Consistency-for-Efficient-Semi-supervised-Learning.md)**|
|**02-05**|**Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?**|W Wu et.al.|[2301.00184](https://arxiv.org/pdf/2301.00184.pdf)|`CVPR'23`|**[](./TLDR/Cap4Video-What-Can-Auxiliary-Captions-Do-for-Text-Video-Retrieval.md)**|
|**02-04**|**Improving fine-grained understanding in image-text pre-training**|I Bica et.al.|[2401.09865](https://arxiv.org/pdf/2401.09865.pdf)|`preprint'24`|**[](./TLDR/Improving-fine-grained-understanding-in-image-text-pre-training.md)**|
|**02-03**|**Erasing Concepts from Diffusion Models**|R Gandikota et.al.|[2303.07345](https://arxiv.org/pdf/2303.07345.pdf)|`ICCV'23`|**[](./TLDR/Erasing-Concepts-from-Diffusion-Models.md)**|
|**02-02**|**TextManiA: Enriching Visual Feature by Text-driven Manifold Augmentation**|M Ye-Bin et.al.|[2307.14611](https://arxiv.org/pdf/2307.14611.pdf)|`ICCV'23`|**[](./TLDR/TextManiA-Enriching-Visual-Feature-by-Text-driven-Manifold-Augmentation.md)**|
|**02-01**|**Zero-shot Referring Image Segmentation with Global-Local Context Features**|S Yu et.al.|[2303.17811](https://arxiv.org/pdf/2303.17811.pdf)|`CVPR'23`|**[](./TLDR/Zero-shot-Referring-Image-Segmentation-with-Global-Local-Context-Features.md)**|
|**01-31**|**On Distillation of Guided Diffusion Models**|C Meng et.al.|[2210.03142](https://arxiv.org/pdf/2210.03142.pdf)|`CVPR'23`|**[](./TLDR/On-Distillation-of-Guided-Diffusion-Models.md)**|
|**01-30**|**SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis**|D Podell et.al.|[2307.01952](https://arxiv.org/pdf/2307.01952.pdf)|`ICLR'23`, `StabilityAI`|**[](./TLDR/SDXL-Improving-Latent-Diffusion-Models-for-High-Resolution-Image-Synthesis.md)**|
|**01-29**|**High-Resolution Image Synthesis with Latent Diffusion Models**|R Rombach et.al.|[2112.10752](https://arxiv.org/pdf/2112.10752.pdf)|``CVPR'22``, |**[](./TLDR/High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models.md)**|
|**01-28**|**DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation**|N Ruiz et.al.|[2208.12242](https://arxiv.org/pdf/2208.12242.pdf)|`CVPR'23`, `Google`|**[](./TLDR/DreamBooth-Fine-Tuning-Text-to-Image-Diffusion-Models-for-Subject-Driven-Generation.md)**|
|**01-27**|**MIC: Masked Image Consistency for Context Enhanced Domain Adaptation**|L Hoyer et.al.|[2212.01322](https://arxiv.org/pdf/2212.01322.pdf)|`CVPR'23`, |**[](./TLDR/MIC-Masked-Image-Consistency-for-Context-Enhanced-Domain-Adaptation.md)**|
|**01-26**|**Tri-Perspective View for Vision Based 3D Semantic Occupancy Prediction**|Y Huang et.al.|[2302.07817](https://arxiv.org/pdf/2302.07817.pdf)|`CVPR'23` |**[](./TLDR/Tri-Perspective-View-for-Vision-Based-3D-Semantic-Occupancy-Prediction.md)**|
|**01-25**|**Cross-modal graph matching network for image-text retrieval**|Y Cheng et.al.|[10.1145/3499027](https://dl.acm.org/doi/abs/10.1145/3499027)|`ACM'22`, |**[](./TLDR/Cross-modal-graph-matching-network-for-image-text-retrieval.md)**|
|**01-24**|**Three things everyone should know about Vision Transformers**|H Touvron et.al.|[2203.09795](https://arxiv.org/pdf/2203.09795.pdf)|`ECCV'22`, |**[](./TLDR/Three-things-everyone-should-know-about-Vision-Transformers.md)**|
|**01-23**|**MetaFormer is Acutally What You Need for Vision**|W Yu et.al.|[2111.11418](https://arxiv.org/pdf/2111.11418.pdf)|`CVPR'22`, |**[](./TLDR/MetaFormer-is-Acutally-What-You-Need-for-Vision.md)**|
|**01-22**|**CLIP: Learning Transferable Visual Models From Natural Language Supervision**|A Radford et.al.|[2103.00020](https://arxiv.org/pdf/2103.00020.pdf)|`ICML'21`, |**[](./TLDR/CLIP-Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.md)**|
|**01-21**|**Variational Adversarial Active Learning**|S Sinha et.al.|[1904.00370](https://arxiv.org/pdf/1904.00370.pdf)|`ICCV'19`, |**[](./TLDR/Variational-Adversarial-Active-Learning.md)**|
|**01-20**|**FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization**|PKA Vasu et.al.|[2303.14189](https://arxiv.org/pdf/2303.14189.pdf)|`ICCV'23`, |**[](./TLDR/FastViT-A-Fast-Hybrid-Vision-Transformer-using-Structural-Reparameterization.md)**|
|**01-19**|**MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures**|Z Chen et.al.|[2208.00277](https://arxiv.org/pdf/2208.00277.pdf)|`CVPR'23` |**[](./TLDR/MobileNeRF-Exploiting-the-Polygon-Rasterization-Pipeline-for-Efficient-Neural-Field-Rendering-on-Mobile-Architectures.md)**|
|**01-18**|**ANYTEXT: MULTILINGUAL VISUAL TEXT GENERATION AND EDITING**|Y Tuo et.al.|[2311.03054](https://arxiv.org/pdf/2311.03054.pdf)|`CVPR'23`, `Diffusion`|**[](./TLDR/ANYTEXT-MULTILINGUAL-VISUAL-TEXT-GENERATION-AND-EDITING.md)**|
|**01-17**|**Visual Programming: Compositional visual reasoning without training**|T Gupta et.al.|[2211.11559](https://arxiv.org/pdf/2211.11559.pdf)|`CVPR'23`, `Vision multi-task`|**[](./TLDR/Visual-Programming-Compositional-visual-reasoning-without-training.md)**|
|**01-16**|**TOKEN MERGING: YOUR VIT BUT FASTER**|D Bolya et.al.|[2210.09461](https://arxiv.org/pdf/2210.09461.pdf)|`ICLR'23`, `ViT`|**[](./TLDR/TOKEN-MERGING-YOUR-VIT-BUT-FASTER.md)**|
|**01-15**|**BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation**|J Li et.al.|[2201.12086](https://arxiv.org/pdf/2201.12086.pdf)|`ICLR'23`, `ViT`|**[](./TLDR/BLIP-Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation.md)**|
|**01-14**|**Gemini: A Family of Highly Capable Multimodal Models**|Gemini Team|[2312.11805](https://arxiv.org/pdf/2312.11805.pdf)|`preprint'23`, `Google`|**[](./TLDR/Gemini-A-Family-of-Highly-Capable-Multimodal-Models.md)**|
|**01-13**|**GPT-4V(ision) System Card**|OpenAI|[GPTV_System_Card](https://cdn.openai.com/papers/GPTV_System_Card.pdf)|`preprint23`, `OpenAI`|**[](./TLDR/GPT-4V(ision)-System-Card.md)**|
|**01-12**|**Prismer: A Vision-Language Model with An Ensemble of Experts**|S Liu et.al.|[2303.02506](https://arxiv.org/pdf/2303.02506.pdf)|`TMLR'24`|**[](./TLDR/Prismer-A-Vision-Language-Model-with-An-Ensemble-of-Experts.md)**|
|**01-11**|**Language Is Not All You Need: Aligning Perception with Language Models**|S Huang et.al.|[2302.14045](https://arxiv.org/pdf/2302.14045.pdf)|`NeurIPS'23`, `Microsoft`|**[](./TLDR/Language-Is-Not-All-You-Need-Aligning-Perception-with-Language-Models.md)**|
|**01-10**|**BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models**|J Li et.al.|[2301.12597](https://arxiv.org/pdf/2301.12597.pdf)|`ICML'23`|**[](./TLDR/BLIP-2-Bootstrapping-Language-Image-Pre-training-with-Frozen-Image-Encoders-and-Large-Language-Models.md)**|
|**01-09**|**VIMA: General Robot Manipulation with Multimodal Prompts**|Y Jiang et.al.|[2210.03094](https://arxiv.org/pdf/2210.03094.pdf)|`ICLR'23`|**[](./TLDR/VIMA-General-Robot-Manipulation-with-Multimodal-Prompts.md)**|
|**01-08**|**Write and Paint: Generative Vision-Language Models are Unified Modal Learners**|S Diao et.al.|[2206.07699](https://arxiv.org/pdf/2206.07699.pdf)|`ICLR'23`|**[](./TLDR/Write-and-Paint-Generative-Vision-Language-Models-are-Unified-Modal-Learners.md)**|
|**01-07**|**MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge**|L Fan et.al.|[2206.08853](https://arxiv.org/pdf/2206.08853.pdf)|`NeurIPS'22`|**[](./TLDR/MineDojo-Building-Open-Ended-Embodied-Agents-with-Internet-Scale-Knowledge.md)**|
|**01-06**|**Language Models are General-Purpose Interfaces**|Y Hao et.al.|[2206.06336](https://arxiv.org/pdf/2206.06336.pdf)|`preprint'22`, `Microsoft`|**[](./TLDR/Language-Models-are-General-Purpose-Interfaces.md)**|
|**01-05**|**Bootstrapping Vision-Language Learning with Decoupled Language Pre-training**|Y Jian et.al.|[2307.07063](https://arxiv.org/pdf/2307.07063.pdf)|`NeurIPS'23`|**[](./TLDR/Bootstrapping-Vision-Language-Learning-with-Decoupled-Language-Pre-training.md)**|
|**01-04**|**Unified Model for Image, Video, Audio and Language Tasks**|M Shukor et.al.|[2307.16184](https://arxiv.org/pdf/2307.16184.pdf)|`TMLR'23`|**[](./TLDR/Unified-Model-for-Image,-Video,-Audio-and-Language-Tasks.md)**|
|**01-03**|**Generative Pretraining in Multimodality**|Q Sun et.al.|[2307.05222](https://arxiv.org/pdf/2307.05222.pdf)|`ICLR'24`|**[](./TLDR/Generative-Pretraining-in-Multimodality.md)**|
|**01-02**|**Kosmos-2: Grounding Multimodal Large Language Models to the World**|Z Peng et.al.|[2306.14824](https://arxiv.org/pdf/2306.14824.pdf)|`ICLR'24`|**[](./TLDR/Kosmos-2-Grounding-Multimodal-Large-Language-Models-to-the-World.md)**|
|**01-01**|**VPGTrans: Transfer Visual Prompt Generator across LLMs**|A Zhang et.al.|[2305.01278](https://arxiv.org/pdf/2305.01278.pdf)|`NeurIPS'23`|**[KOR](./TLDR/Transfer-Visual-Prompt-Generator-across-LLMs.md)**|