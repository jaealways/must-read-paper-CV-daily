# must-read-paper-CV-daily
List of papers with recent trend in CV

## 2023
- [2023 CVPR](/2023/CVPR.md)



## Updated on 2024.02.14

# Must-read LIST

|Date|Title|Authors|PDF|TAGS|TLDR|
|---|---|---|---|---|---|
|**02-24**|**An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA**|Z Yang et.al.|[2109.05014](https://arxiv.org/pdf/2109.05014.pdf)|AAAI'22, Microsoft, multi-modal|**[]()**|
|**02-23**|**DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models**| et.al.|[ ]( )|NeurIPS|**[]()**|
|**02-22**|**Multimodal Few-Shot Learning with Frozen Language Models**| et.al.|[ ]( )|NeurIPS|**[]()**|
|**02-21**|**Flamingo: a Visual Language Model for Few-Shot Learning**| et.al.|[ ]( )|NeurIPS|**[]()**|
|**02-20**|**An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA**| et.al.|[ ]( )|AAAI|**[]()**|
|**02-19**|**Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering**| et.al.|[ ]( )|CVPR|**[]()**|
|**02-18**|**ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction**| et.al.|[ ]( )|ICCV|**[]()**|
|**02-17**|**Exploring Diverse In-Context Configurations for Image Captioning**| et.al.|[ ]( )|NeurIPS|**[]()**|
|**02-16**|**MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning**| et.al.|[ ]( )|ACL|**[]()**|
|**02-15**|**LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention**| et.al.|[ ]( )|ICLR|**[]()**|
|**02-14**|**Visual Instruction Tuning**| et.al.|[ ]( )|NeurIPS'23|**[]()**|
|**02-13**|**Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages**| et.al.|[ ]( )|ICLR|**[]()**|
|**02-12**|**LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment**| et.al.|[ ]( )|ICLR|**[]()**|
|**02-11**|**Multimodal Representation Learning: Perks and Pitfalls**| et.al.|[ ]( )|ICLR'23|**[]()**|
|**02-10**|**MMVAE+: Enhancing the Generative Quality of Multimodal VAEs without Compromises**| et.al.|[ ]( )|ICLR'23|**[]()**|
|**02-09**|**Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study**| et.al.|[ ]( )|ECIR'23|**[]()**|
|**02-08**|**Identifiability Results for Multimodal Contrastive Learning**| et.al.|[ ]( )|ICLR'23|**[]()**|
|**02-07**|**UniCLIP Unified Framework for Contrastive Language Image Pre training**| et.al.|[ ]( )| |**[]()**|
|**02-06**|**ProtoCon: Pseudo-label Refinement via Online Clustering and Prototypical Consistency for Efficient Semi-supervised Learning**| et.al.|[ ]( )|CVPR'23|**[]()**|
|**02-05**|**Zero-shot Referring Image Segmentation with Global-Local Context Features**| et.al.|[ ]( )|CVPR'23|**[]()**|
|**02-04**|**Improving fine-grained understanding in image-text pre-training**| et.al.|[ ]( )|preprint|**[]()**|
|**02-03**|**Erasing Concepts from Diffusion Models**| et.al.|[ ]( )|ICCV'23|**[]()**|
|**02-02**|**TextManiA: Enriching Visual Feature by Text-driven Manifold Augmentation**| et.al.|[ ]( )|ICCV'23|**[]()**|
|**02-01**|**Zero-shot Referring Image Segmentation with Global-Local Context Features**| et.al.|[ ]( )|CVPR'23|**[]()**|
|**01-31**|**On Distillation of Guided Diffusion Models**| et.al.|[ ]( )| |**[]()**|
|**01-30**|**SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis**|Z Chen et.al.|[2208.00277](https://arxiv.org/pdf/2208.00277.pdf)|CVPR'23, 3D reconstruction|
|**01-29**|**High-Resolution Image Synthesis with Latent Diffusion Models**|Z Chen et.al.|[2208.00277](https://arxiv.org/pdf/2208.00277.pdf)|CVPR'22, 3D reconstruction|**[]()**|
|**01-28**|**DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation**| et.al.|[ ]( )| |**[]()**|
|**01-27**|**MIC: Masked Image Consistency for Context Enhanced Domain Adaptation**| et.al.|[ ]( )| |**[]()**|
|**01-26**|**Tri-Perspective View for Vision Based 3D Semantic Occupancy Prediction**| et.al.|[ ]( )| |**[]()**|
|**01-25**|**Cross-modal graph matching network for image-text retrieval**|Z Chen et.al.|[2208.00277](https://arxiv.org/pdf/2208.00277.pdf)|ACM'22, 3D reconstruction|**[]()**|
|**01-24**|**Three things everyone should know about Vision Transformers**|Z Chen et.al.|[2208.00277](https://arxiv.org/pdf/2208.00277.pdf)|ECCV'22, 3D reconstruction|**[]()**|
|**01-23**|**MetaFormer is Acutally What You Need for Vision**|Z Chen et.al.|[2208.00277](https://arxiv.org/pdf/2208.00277.pdf)|CVPR'22, 3D reconstruction|**[]()**|
|**01-22**|**CLIP: Learning Transferable Visual Models From Natural Language Supervision**|A Radford et.al.|[2208.00277](https://arxiv.org/pdf/2208.00277.pdf)|ICML'21, 3D reconstruction|**[]()**|
|**01-21**|**Variational Adversarial Active Learning**|Z Chen et.al.|[2208.00277](https://arxiv.org/pdf/2208.00277.pdf)|ICCV'19, 3D reconstruction|**[]()**|
|**01-20**|**FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization**|Z Chen et.al.|[2208.00277](https://arxiv.org/pdf/2208.00277.pdf)|ICCV'23, 3D reconstruction|**[]()**|
|**01-19**|**MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures**| et.al.|[ ]( )| |**[]()**|
|**01-18**|**ANYTEXT: MULTILINGUAL VISUAL TEXT GENERA-TION AND EDITING**|Y Tuo et.al.|[2311.03054](https://arxiv.org/pdf/2311.03054.pdf)|CVPR'23, Diffusion|**[]()**|
|**01-17**|**Visual Programming: Compositional visual reasoning without training**|T Gupta et.al.|[2211.11559](https://arxiv.org/pdf/2211.11559.pdf)|CVPR'23, Vision multi-task|**[]()**|
|**01-16**|**TOKEN MERGING: YOUR VIT BUT FASTER**|D Bolya et.al.|[2210.09461](https://arxiv.org/pdf/2210.09461.pdf)|ICLR'23, ViT|**[]()**|
|**01-15**|**BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation**|J Li et.al.|[2201.12086](https://arxiv.org/pdf/2201.12086.pdf)|ICLR'23, ViT|**[]()**|
|**01-14**|**Gemini: A Family of Highly Capable Multimodal Models**| et.al.|[ ]( )|Google|**[]()**|
|**01-13**|**GPT-4V(ision) System Card**| et.al.|[ ]( )|OpenAI|**[]()**|
|**01-12**|**Prismer: A Vision-Language Model with An Ensemble of Experts**| et.al.|[ ]( )| |**[]()**|
|**01-11**|**Language Is Not All You Need: Aligning Perception with Language Models**| et.al.|[ ]( )| |**[]()**|
|**01-10**|**BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models**| et.al.|[ ]( )| |**[]()**|
|**01-09**|**VIMA: General Robot Manipulation with Multimodal Prompts**| et.al.|[ ]( )|ICML|**[]()**|
|**01-08**|**Write and Paint: Generative Vision-Language Models are Unified Modal Learners**| et.al.|[ ]( )|ICLR|**[]()**|
|**01-07**|**MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge**| et.al.|[ ]( )|NeurIPS|**[]()**|
|**01-06**|**Language Models are General-Purpose Interfaces**| et.al.|[ ]( )| |**[]()**|
|**01-05**|**Bootstrapping Vision-Language Learning with Decoupled Language Pre-training**| et.al.|[ ]( )|NeurIPS|**[]()**|
|**01-04**|**Unified Model for Image, Video, Audio and Language Tasks**| et.al.|[ ]( )| |**[]()**|
|**01-03**|**Generative Pretraining in Multimodality**| et.al.|[ ]( )| |**[]()**|
|**01-02**|**Kosmos-2: Grounding Multimodal Large Language Models to the World**| et.al.|[ ]( )| |**[]()**|
|**01-01**|**Transfer Visual Prompt Generator across LLMs**| et.al.|[ ]( )| |**[]()**|